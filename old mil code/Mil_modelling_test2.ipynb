{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Calling packages and assigning variables\n",
    "Here i call the necessary packages as well as assigning variables. Of note are the paths to collected data which will need to be changed for replication in another system"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "if not os.path.isdir(\"src/misvm\"):\n",
    "    !pip install -e git+https://github.com/garydoranjr/misvm.git#egg=misvm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import pandas as pd\n",
    "import misvm\n",
    "import rdkit\n",
    "import numpy as np\n",
    "import pickle\n",
    "from ast import literal_eval\n",
    "from tpot import TPOTClassifier\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MACCSkeys\n",
    "from mordred import Calculator, descriptors\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "data = pd.read_csv(\"/home/samuel/honours_redo/1-data/selected_molecules.csv\")\n",
    "metabolite_data = pd.read_csv(\"/home/samuel/honours_redo/publication_work/biotransformer_output_phaseII.csv\").append(pd.read_csv(\"/home/samuel/honours_redo/publication_work/biotransformer_output_cyp1.csv\"))\n",
    "\n",
    "# validation_data = pd.read_csv()\n",
    "# validation_metabolite_data = pd.read_csv()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cleaning Biotransformer data\n",
    "This section is to ensure that metabolites label the correct parent molecule as the root parent and not the direct one. Simply it replaces the \"parent molecule\" if that molecule is a metabolite and records the parent of said molecule.\n",
    "\n",
    "Additionally i encode all the molecules with the expected methods here to reduce calculation times in iterations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def normalize_smiles(smi):\n",
    "    try:\n",
    "        smi_norm = Chem.MolToSmiles(Chem.MolFromSmiles(smi))\n",
    "        return smi_norm\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def parent_finder(smi):\n",
    "    for parent in data['smiles']:\n",
    "        try:\n",
    "            if Chem.MolToSmiles(Chem.MolFromSmiles(smi)) == Chem.MolToSmiles(Chem.MolFromSmiles(parent)):\n",
    "                return parent\n",
    "        except:\n",
    "            continue\n",
    "    return \"No parent found\"\n",
    "\n",
    "def number_check(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return \"broken\"\n",
    "\n",
    "def get_ml_fingerprint(df, function=MACCSkeys.GenMACCSKeys):                                                                    \n",
    "    df1 = df.copy()                                                                     \n",
    "    df1['fp_list'] = df1['smiles'].apply(lambda x: list(function(Chem.MolFromSmiles(x))))     \n",
    "    df1 = df1.dropna(axis = 1, how = 'any')                                             \n",
    "\n",
    "    df2 = pd.DataFrame(df1['fp_list'].to_list())                                        \n",
    "    df2 = df2.applymap(number_check).dropna(axis =1, how = \"any\")                       \n",
    "    df2 = df2.drop(columns=df2.columns[(df2 == 'broken').any()])                        \n",
    "    X = [[int(i) for i in lst] for lst in df2.values.tolist()]                                                                                                                          \n",
    "    return X\n",
    "\n",
    "def bag_parent(smiles,met_df,function):\n",
    "    mol_family          =   met_df[met_df[\"parent smiles\"]==smiles].append({'smiles':smiles},ignore_index=True).drop_duplicates(subset=[\"smiles\"])\n",
    "    mol_family_encoded  =   get_ml_fingerprint(df = mol_family, function = function)\n",
    "    return mol_family_encoded"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "## I precalculate the splits and the encoded molecules to save time restarting calculations. This also acts as a checkpoint to allow for checking the data into the model\n",
    "\n",
    "if os.path.isfile(\"train_test.pk1\"):\n",
    "    print(\"Crossvalidation indices already determined in file: train_test.csv\")\n",
    "    print('To redo them please delete these files and for a different set please choose another random stat or \"none\" within the code block above')\n",
    "else:\n",
    "    ##          Step 1: defining the splits\n",
    "    skf = StratifiedKFold(n_splits=10, random_state=871923, shuffle=True)\n",
    "    First = True\n",
    "    for itr in range(1,11):\n",
    "        for fold, [train_index, test_index] in enumerate(skf.split(data,data['Ames'])):\n",
    "            fold +=1\n",
    "            if First:\n",
    "                test_df = pd.DataFrame({\"index\":test_index})\n",
    "                train_df = pd.DataFrame({\"index\":train_index})\n",
    "                test_df[\"group\"] = \"test\"; train_df[\"group\"] = \"train\"\n",
    "                test_df[\"fold\"] = fold; train_df[\"fold\"] = fold\n",
    "                test_df[\"iteration\"] = itr; train_df[\"iteration\"] = itr\n",
    "                train_test = test_df.append(train_df)\n",
    "                First = False\n",
    "            else:\n",
    "                test_df = pd.DataFrame({\"index\":test_index})\n",
    "                train_df = pd.DataFrame({\"index\":train_index})\n",
    "                test_df[\"group\"] = \"test\"; train_df[\"group\"] = \"train\"\n",
    "                test_df[\"fold\"] = fold; train_df[\"fold\"] = fold\n",
    "                test_df[\"iteration\"] = itr; train_df[\"iteration\"] = itr\n",
    "                train_test = train_test.append(test_df).append(train_df)\n",
    "\n",
    "    ##          Step 2: Normailizing metabolite smiles and matching to parent (approx 220 secs) \n",
    "    metabolite_data['smiles'] = metabolite_data['SMILES'].apply(lambda x: normalize_smiles(x))\n",
    "    metabolite_data = metabolite_data.dropna(axis=0,subset=['smiles'])\n",
    "    metabolite_data['parent smiles'] = metabolite_data['Precursor SMILES'].apply(lambda x:parent_finder(x))\n",
    "\n",
    "    ##          Step 3: Pre calculating encoding for molecules, requires evaluation of lists on loading csv (approx 110 secs)\n",
    "    data[\"MACCS\"] = get_ml_fingerprint(df = data, function = MACCSkeys.GenMACCSKeys)\n",
    "    data[\"RDKF\"] = get_ml_fingerprint(df = data, function =  Chem.RDKFingerprint)\n",
    "    data[\"MACCS_MIL\"] = data.apply(lambda row: bag_parent(smiles = row['smiles'], met_df = metabolite_data, function = MACCSkeys.GenMACCSKeys),axis=1)\n",
    "    data[\"RDKF_MIL\"] = data.apply(lambda row: bag_parent(smiles = row['smiles'], met_df = metabolite_data, function = Chem.RDKFingerprint),axis=1)\n",
    "    train_test = pd.merge(train_test, data[[\"smiles\",\"Ames\",\"MACCS\",\"RDKF\",\"MACCS_MIL\",\"RDKF_MIL\",\"idx\"]], left_on=\"index\",right_on=\"idx\").drop(\"idx\",axis=1)\n",
    "    \n",
    "    ##          Step 4: Saved to a pickle, rather than a csv this stores the lists and is much faster to load (~10x)\n",
    "    train_test.to_pickle(\"train_test.pk1\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Crossvalidation indices already determined in file: train_test.csv\n",
      "To redo them please delete these files and for a different set please choose another random stat or \"none\" within the code block above\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining functions\n",
    "These functions are used to execute the described actions on the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "train_test = pd.read_pickle(\"train_test.pk1\")\n",
    "\n",
    "testing = train_test[(train_test['group']==\"train\") & (train_test[\"fold\"]==1) & (train_test[\"iteration\"]==1)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def build_mil_model(training_data,MIL,fingerprint,name):\n",
    "    if not os.path.isfile(\"saved_models/\"+name):\n",
    "            bags = training_data[fingerprint+\"_MIL\"].to_list()\n",
    "            labels = training_data[\"Ames\"].apply(lambda x: x if x==1 else -1).to_list()\n",
    "            model = MIL                                                                 \n",
    "            model.fit(bags,labels)                                                      \n",
    "            pickle.dump(model, open(\"saved_models/\"+name, 'wb'))                        \n",
    "    else:\n",
    "        print(name,\"is already built\")\n",
    "\n",
    "def build_ml_model(training_data,fingerprint,ML,name):\n",
    "    if not os.path.isfile(\"saved_models/\"+name):\n",
    "        instances = training_data[fingerprint].to_list()\n",
    "        labels = training_data[\"Ames\"].to_list()       \n",
    "        tpot_optimisation = ML                                                          \n",
    "        tpot_optimisation.fit(instances,labels)                                         \n",
    "        model = tpot_optimisation.fitted_pipeline_                                      \n",
    "        pickle.dump(model, open(\"saved_models/\"+name, 'wb'))                             \n",
    "    else:\n",
    "        print(name,\"is already built\")\n",
    "\n",
    "def test_mil_model(testing_data,grouping_data,fingerprint,name):\n",
    "    if not os.path.isfile(\"saved_tests/\"+name.split(\".\")[0]+\".csv\"):\n",
    "        loaded_model = pickle.load(open(\"saved_models/\"+name, 'rb'))                    \n",
    "        bags = training_data[fingerprint+\"_MIL\"].to_list()\n",
    "        labels = training_data[\"Ames\"].apply(lambda x: x if x==1 else -1).to_list()\n",
    "        predictions = loaded_model.predict(bags)                                        \n",
    "        predicted_labels = list(map(pos_or_neg,predictions))                            \n",
    "        df = pd.DataFrame({\n",
    "            'predicted' : predictions,\n",
    "            'predicted labal' : predicted_labels,\n",
    "            'true label' : true_labels\n",
    "        })                                                                              \n",
    "        df.to_csv(\"saved_tests/\"+name.split(\".\")[0]+\".csv\",index=False )                \n",
    "    else:\n",
    "        print(name,\"is already built\")\n",
    "\n",
    "def test_ml_model(testing_data,fingerprint,name):\n",
    "    if not os.path.isfile(\"saved_tests/\"+name.split(\".\")[0]+\".csv\"):\n",
    "        loaded_model = pickle.load(open(\"saved_models/\"+name, 'rb'))                    \n",
    "        instances = training_data[fingerprint].to_list()\n",
    "        labels = training_data[\"Ames\"].to_list()       \n",
    "        predictions = loaded_model.predict(instances)                                   \n",
    "        predicted_labels = list(map(pos_or_neg,predictions))                            \n",
    "        df = pd.DataFrame({\n",
    "            'predicted' : predictions,\n",
    "            'predicted labal' : predicted_labels,\n",
    "            'true label' : true_labels\n",
    "        })                                                                              \n",
    "        df.to_csv(\"saved_tests/\"+name.split(\".\")[0]+\".csv\",index=False )                \n",
    "    else:\n",
    "        print(name,\"is already built\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def build_mil_model(training_data,grouping_data,fingerprint,MIL,name):\n",
    "    if not os.path.isfile(\"saved_models/\"+name):\n",
    "            [bags,labels] = generate_bags(training_data,grouping_data,fingerprint)      \n",
    "            model = MIL                                                                 \n",
    "            model.fit(bags,labels)                                                      \n",
    "            pickle.dump(model, open(\"saved_models/\"+name, 'wb'))                        \n",
    "    else:\n",
    "        print(name,\"is already built\")\n",
    "\n",
    "def build_ml_model(training_data,fingerprint,ML,name):\n",
    "    if not os.path.isfile(\"saved_models/\"+name):\n",
    "        [instances,labels] = get_ml_fingerprint(training_data, fingerprint)             \n",
    "        tpot_optimisation = ML                                                          \n",
    "        tpot_optimisation.fit(instances,labels)                                         \n",
    "        model = tpot_optimisation.fitted_pipeline_                                      \n",
    "        pickle.dump(model, open(\"saved_models/\"+name, 'wb'))                             \n",
    "    else:\n",
    "        print(name,\"is already built\")\n",
    "\n",
    "def test_mil_model(testing_data,grouping_data,fingerprint,name):\n",
    "    if not os.path.isfile(\"saved_tests/\"+name.split(\".\")[0]+\".csv\"):\n",
    "        loaded_model = pickle.load(open(\"saved_models/\"+name, 'rb'))                    \n",
    "        [bags,true_labels] = generate_bags(testing_data,grouping_data,fingerprint)      \n",
    "        predictions = loaded_model.predict(bags)                                        \n",
    "        predicted_labels = list(map(pos_or_neg,predictions))                            \n",
    "        df = pd.DataFrame({\n",
    "            'predicted' : predictions,\n",
    "            'predicted labal' : predicted_labels,\n",
    "            'true label' : true_labels\n",
    "        })                                                                              \n",
    "        df.to_csv(\"saved_tests/\"+name.split(\".\")[0]+\".csv\",index=False )                \n",
    "    else:\n",
    "        print(name,\"is already built\")\n",
    "\n",
    "def test_ml_model(testing_data,fingerprint,name):\n",
    "    if not os.path.isfile(\"saved_tests/\"+name.split(\".\")[0]+\".csv\"):\n",
    "        loaded_model = pickle.load(open(\"saved_models/\"+name, 'rb'))                    \n",
    "        [instances,true_labels] = get_ml_fingerprint(testing_data, fingerprint)         \n",
    "        predictions = loaded_model.predict(instances)                                   \n",
    "        predicted_labels = list(map(pos_or_neg,predictions))                            \n",
    "        df = pd.DataFrame({\n",
    "            'predicted' : predictions,\n",
    "            'predicted labal' : predicted_labels,\n",
    "            'true label' : true_labels\n",
    "        })                                                                              \n",
    "        df.to_csv(\"saved_tests/\"+name.split(\".\")[0]+\".csv\",index=False )                \n",
    "    else:\n",
    "        print(name,\"is already built\")\n",
    "\n",
    "# def generate_bags(df,bt_df, funct):\n",
    "#         bags = []; labels = []                                                          \n",
    "#         for smile in df['smiles'].to_list():\n",
    "#             wk_data =   bt_df[bt_df[\"parent smiles\"]==smile]                            \n",
    "#             if not wk_data.empty:\n",
    "#                 wk_data = get_mil_fingerprint(wk_data, funct)\n",
    "#                 bags        += [np.array(wk_data['fp_list'].to_list())]                 \n",
    "#                 labels      += [df.loc[df['smiles'] == smile, 'Ames'].item()]           \n",
    "#         for i,x in enumerate(labels):\n",
    "#             if x == 0:\n",
    "#                 labels[i] = -1                                                          \n",
    "#         bags = np.array(bags)                                                           \n",
    "#         labels = np.array(labels)                                                       \n",
    "#         return [bags, labels]                                                           \n",
    "\n",
    "# def get_mil_fingerprint(df, function):\n",
    "#     fn = function                                                                       \n",
    "#     df1 = df.copy()                                                                     \n",
    "#     df1['fp_list'] = df1['smiles'].apply(lambda x: list(fn(Chem.MolFromSmiles(x))))\n",
    "#     return df1\n",
    "\n",
    "# def pos_or_neg(x):\n",
    "#     if x>0:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return -1\n",
    "    \n",
    "# def number_check(x):\n",
    "#     try:\n",
    "#         return float(x)\n",
    "#     except:\n",
    "#         return \"broken\"\n",
    "\n",
    "# def get_ml_fingerprint(df, function):\n",
    "#     if function != \"Morgan\":\n",
    "#         fn = function                                                                       \n",
    "#         df1 = df.copy()                                                                     \n",
    "#         df1['fp_list'] = df1['smiles'].apply(lambda x: list(fn(Chem.MolFromSmiles(x))))     \n",
    "#         df1 = df1.dropna(axis = 1, how = 'any')                                             \n",
    "\n",
    "#         df2 = pd.DataFrame(df1['fp_list'].to_list())                                        \n",
    "#         df2 = df2.applymap(number_check).dropna(axis =1, how = \"any\")                       \n",
    "#         df2 = df2.drop(columns=df2.columns[(df2 == 'broken').any()])                        \n",
    "#         df1['fp_list'] = df2.values.tolist()                                                \n",
    "#         X = np.array(df1['fp_list'].to_list())                                              \n",
    "#         Y = np.array(df1['Ames'].to_list())                                                 \n",
    "#         return [X,Y]\n",
    "#     else:\n",
    "#         radius = 3                                                                          \n",
    "#         df1 = df.copy()                                                                     \n",
    "#         df1['fp_list'] = df1['smiles'].apply(lambda x: list(AllChem.GetMorganFingerprint(Chem.MolFromSmiles(x),radius).ToBinary()))     \n",
    "#         df2 = pd.DataFrame(df1['fp_list'].to_list())                                        \n",
    "#         df2 = df2.applymap(number_check).dropna(axis =1, how = \"any\")                       \n",
    "#         df2 = df2.drop(columns=df2.columns[(df2 == 'broken').any()])\n",
    "#         df1['fp_list'] = df2.values.tolist()\n",
    "#         X = np.array(df1['fp_list'].to_list())\n",
    "#         Y = np.array(df1['Ames'].to_list())\n",
    "#         return [X,Y]\n",
    "\n",
    "\n",
    "# def clearConsole():\n",
    "#     command = 'clear'\n",
    "#     if os.name in ('nt', 'dos'):  # If Machine is running on Windows, use cls\n",
    "#         command = 'cls'\n",
    "#     os.system(command)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building models\n",
    "Here the above functions are used to build models. This section can be altered to build additional models if desired"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "def develop_models(training_data,training_data_metabolites,testing_data,testing_data_metabolites,suffix=\"\",encoding=\"MACCS\"):\n",
    "    tested_mils =  [[\"MICA\", misvm.MICA(max_iters=50,verbose=False)],     \n",
    "                [\"MISVM\", misvm.MISVM(kernel='linear', C=1.0, max_iters=50,verbose=False)],\n",
    "                ['SIL', misvm.SIL(verbose=False)],\n",
    "                ['NSK', misvm.NSK(verbose=False)],\n",
    "                ['sMIL', misvm.sMIL(verbose=False)]]\n",
    "\n",
    "    fps = {\"MACCS\":MACCSkeys.GenMACCSKeys,\"RDFP\":Chem.RDKFingerprint}\n",
    "    if encoding in fps:\n",
    "        fp = fps[encoding]\n",
    "    else:\n",
    "        print('Please use expected fingerprint: [\"MACCS\", \"RDFP\"]')\n",
    "        return\n",
    "    \n",
    "    # Iterate over the used MILs\n",
    "    for mil in tested_mils:\n",
    "        clearConsole();print(\"     Building MIL model:\",mil[0],\"    fold:\",suffix.split(\"_iteration\")[0].split(\"fold\")[-1],\"    Iteration:\",suffix.split(\"iteration\")[-1])\n",
    "        build_mil_model(training_data=training_data,grouping_data=training_data_metabolites,name=encoding+\"_\"+mil[0]+suffix+\".sav\",MIL=mil[1],fingerprint=fp)\n",
    "        \n",
    "        clearConsole();print(\"     Testing MIL model:\",mil[0],\"    fold:\",suffix.split(\"_iteration\")[0].split(\"fold\")[-1],\"    Iteration:\",suffix.split(\"iteration\")[-1])\n",
    "        test_mil_model(testing_data=testing_data,grouping_data=testing_data_metabolites,name=encoding+\"_\"+mil[0]+suffix+\".sav\",fingerprint=fp)\n",
    "    \n",
    "    # Build and test TPOT model\n",
    "    clearConsole();print(\"     Building TPOT model\",\"    fold:\",suffix.split(\"_iteration\")[0].split(\"fold\")[-1],\"    Iteration:\",suffix.split(\"iteration\")[-1])\n",
    "    build_ml_model(training_data=training_data, ML = TPOTClassifier(generations=10, population_size=100, cv=5, random_state=42, verbosity=1),fingerprint=fp,name=encoding+\"_tpot\"+suffix+'.sav')\n",
    "    \n",
    "    clearConsole();print(\"     Testing TPOT model\",\"    fold:\",suffix.split(\"_iteration\")[0].split(\"fold\")[-1],\"    Iteration:\",suffix.split(\"iteration\")[-1])\n",
    "    test_ml_model(testing_data=testing_data,name=encoding+\"_tpot\"+suffix+'.sav',fingerprint=fp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "## For precaution i save the indexes of the models into each crossvalidation. This is also usefull incase the code times out part way through, in addition to a set random state\n",
    "\n",
    "if os.path.isfile(\"train_test.csv\"):\n",
    "    print(\"Crossvalidation indices already determined in file: train_test.csv\")\n",
    "    print('To redo them please delete these files and for a different set please choose another random stat or \"none\" within the code block above')\n",
    "else:\n",
    "    skf = StratifiedKFold(n_splits=10, random_state=871923, shuffle=True)\n",
    "    First = True\n",
    "    for itr in range(1,11):\n",
    "        for fold, [train_index, test_index] in enumerate(skf.split(data,data['Ames'])):\n",
    "            fold +=1\n",
    "            if First:\n",
    "                test_df = pd.DataFrame({\"index\":test_index})\n",
    "                train_df = pd.DataFrame({\"index\":train_index})\n",
    "                test_df[\"group\"] = \"test\"; train_df[\"group\"] = \"train\"\n",
    "                test_df[\"fold\"] = fold; train_df[\"fold\"] = fold\n",
    "                test_df[\"iteration\"] = itr; train_df[\"iteration\"] = itr\n",
    "                train_test = test_df.append(train_df)\n",
    "                First = False\n",
    "            else:\n",
    "                test_df = pd.DataFrame({\"index\":test_index})\n",
    "                train_df = pd.DataFrame({\"index\":train_index})\n",
    "                test_df[\"group\"] = \"test\"; train_df[\"group\"] = \"train\"\n",
    "                test_df[\"fold\"] = fold; train_df[\"fold\"] = fold\n",
    "                test_df[\"iteration\"] = itr; train_df[\"iteration\"] = itr\n",
    "                train_test = train_test.append(test_df).append(train_df)\n",
    "    train_test.to_csv(\"train_test.csv\",index=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Crossvalidation indices already determined in file: train_test.csv\n",
      "To redo them please delete these files and for a different set please choose another random stat or \"none\" within the code block above\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "train_test = pd.read_csv(\"train_test.csv\")\n",
    "for iteration in train_test[\"iteration\"].unique():  \n",
    "    for fold in train_test[\"fold\"].unique():\n",
    "        train_index = train_test[(train_test[\"group\"] == \"train\") & (train_test[\"fold\"] == fold) & (train_test[\"iteration\"] == iteration)][\"index\"].to_list()\n",
    "        test_index = train_test[(train_test[\"group\"] == \"test\") & (train_test[\"fold\"] == fold) & (train_test[\"iteration\"] == iteration)][\"index\"].to_list()\n",
    "        train = data.iloc[train_index]\n",
    "        test =  data.iloc[test_index]\n",
    "        develop_models(train,metabolite_data,test,metabolite_data,suffix='_kfold'+str(fold)+\"_iteration\"+str(iteration),encoding = \"MACCS\")\n",
    "        print(\"Done Fold\", fold)\n",
    "    print(\"Done Iteration\", iteration)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     Building MIL model: MICA     fold: 1     Iteration: 1\n",
      "MACCS_MICA_kfold1_iteration1.sav is already built\n",
      "     Testing MIL model: MICA     fold: 1     Iteration: 1\n",
      "MACCS_MICA_kfold1_iteration1.sav is already built\n",
      "     Building MIL model: MISVM     fold: 1     Iteration: 1\n",
      "MACCS_MISVM_kfold1_iteration1.sav is already built\n",
      "     Testing MIL model: MISVM     fold: 1     Iteration: 1\n",
      "MACCS_MISVM_kfold1_iteration1.sav is already built\n",
      "     Building MIL model: SIL     fold: 1     Iteration: 1\n",
      "MACCS_SIL_kfold1_iteration1.sav is already built\n",
      "     Testing MIL model: SIL     fold: 1     Iteration: 1\n",
      "MACCS_SIL_kfold1_iteration1.sav is already built\n",
      "     Building MIL model: NSK     fold: 1     Iteration: 1\n",
      "MACCS_NSK_kfold1_iteration1.sav is already built\n",
      "     Testing MIL model: NSK     fold: 1     Iteration: 1\n",
      "MACCS_NSK_kfold1_iteration1.sav is already built\n",
      "     Building MIL model: sMIL     fold: 1     Iteration: 1\n",
      "MACCS_sMIL_kfold1_iteration1.sav is already built\n",
      "     Testing MIL model: sMIL     fold: 1     Iteration: 1\n",
      "MACCS_sMIL_kfold1_iteration1.sav is already built\n",
      "     Building TPOT model     fold: 1     Iteration: 1\n",
      "MACCS_tpot_kfold1_iteration1.sav is already built\n",
      "     Testing TPOT model     fold: 1     Iteration: 1\n",
      "MACCS_tpot_kfold1_iteration1.sav is already built\n",
      "Done Fold 1\n",
      "     Building MIL model: MICA     fold: 2     Iteration: 1\n",
      "MACCS_MICA_kfold2_iteration1.sav is already built\n",
      "     Testing MIL model: MICA     fold: 2     Iteration: 1\n",
      "MACCS_MICA_kfold2_iteration1.sav is already built\n",
      "     Building MIL model: MISVM     fold: 2     Iteration: 1\n",
      "MACCS_MISVM_kfold2_iteration1.sav is already built\n",
      "     Testing MIL model: MISVM     fold: 2     Iteration: 1\n",
      "MACCS_MISVM_kfold2_iteration1.sav is already built\n",
      "     Building MIL model: SIL     fold: 2     Iteration: 1\n",
      "MACCS_SIL_kfold2_iteration1.sav is already built\n",
      "     Testing MIL model: SIL     fold: 2     Iteration: 1\n",
      "MACCS_SIL_kfold2_iteration1.sav is already built\n",
      "     Building MIL model: NSK     fold: 2     Iteration: 1\n",
      "MACCS_NSK_kfold2_iteration1.sav is already built\n",
      "     Testing MIL model: NSK     fold: 2     Iteration: 1\n",
      "MACCS_NSK_kfold2_iteration1.sav is already built\n",
      "     Building MIL model: sMIL     fold: 2     Iteration: 1\n",
      "MACCS_sMIL_kfold2_iteration1.sav is already built\n",
      "     Testing MIL model: sMIL     fold: 2     Iteration: 1\n",
      "MACCS_sMIL_kfold2_iteration1.sav is already built\n",
      "     Building TPOT model     fold: 2     Iteration: 1\n",
      "MACCS_tpot_kfold2_iteration1.sav is already built\n",
      "     Testing TPOT model     fold: 2     Iteration: 1\n",
      "MACCS_tpot_kfold2_iteration1.sav is already built\n",
      "Done Fold 2\n",
      "     Building MIL model: MICA     fold: 3     Iteration: 1\n",
      "MACCS_MICA_kfold3_iteration1.sav is already built\n",
      "     Testing MIL model: MICA     fold: 3     Iteration: 1\n",
      "MACCS_MICA_kfold3_iteration1.sav is already built\n",
      "     Building MIL model: MISVM     fold: 3     Iteration: 1\n",
      "MACCS_MISVM_kfold3_iteration1.sav is already built\n",
      "     Testing MIL model: MISVM     fold: 3     Iteration: 1\n",
      "MACCS_MISVM_kfold3_iteration1.sav is already built\n",
      "     Building MIL model: SIL     fold: 3     Iteration: 1\n",
      "MACCS_SIL_kfold3_iteration1.sav is already built\n",
      "     Testing MIL model: SIL     fold: 3     Iteration: 1\n",
      "MACCS_SIL_kfold3_iteration1.sav is already built\n",
      "     Building MIL model: NSK     fold: 3     Iteration: 1\n",
      "MACCS_NSK_kfold3_iteration1.sav is already built\n",
      "     Testing MIL model: NSK     fold: 3     Iteration: 1\n",
      "MACCS_NSK_kfold3_iteration1.sav is already built\n",
      "     Building MIL model: sMIL     fold: 3     Iteration: 1\n",
      "MACCS_sMIL_kfold3_iteration1.sav is already built\n",
      "     Testing MIL model: sMIL     fold: 3     Iteration: 1\n",
      "MACCS_sMIL_kfold3_iteration1.sav is already built\n",
      "     Building TPOT model     fold: 3     Iteration: 1\n",
      "MACCS_tpot_kfold3_iteration1.sav is already built\n",
      "     Testing TPOT model     fold: 3     Iteration: 1\n",
      "MACCS_tpot_kfold3_iteration1.sav is already built\n",
      "Done Fold 3\n",
      "     Building MIL model: MICA     fold: 4     Iteration: 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-4-845cd98a5469>:61: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  bags = np.array(bags)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Validation\n",
    "Here the model results are assessed"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# develop_models(data,bt_data,validation_data,validation_metabolite_data,suffix=\"_validation\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Analysis\n",
    "Here the results of each fold are calculated as well as deviation within crossvalidation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def confusion_matrix(df):\n",
    "    TP = len(df[(df[\"predicted label\"] == 1) & (df[\"true label\"] == 1)])\n",
    "    TN = len(df[(df[\"predicted label\"] == 0) & (df[\"true label\"] == 0)])\n",
    "    FP = len(df[(df[\"predicted label\"] == 1) & (df[\"true label\"] == 0)])\n",
    "    FN = len(df[(df[\"predicted label\"] == 0) & (df[\"true label\"] == 1)])\n",
    "    return [TP,TN,FP,FN]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rslt_list = []\n",
    "\n",
    "for filename in os.listdir(\"/home/samuel/honours_redo/publication_work/saved_tests\"):\n",
    "    [TP,TN,FP,FN] = confusion_matrix(pd.read_csv(filename))\n",
    "    fingerprint = filename.split(\"_\")[0]\n",
    "    model = filename.split(\"_\")[0]\n",
    "    fold = filename.split(\"_\")[-1].split(\".\")[0]\n",
    "    rslt_list += {\"fingerprint\":fingerprint, \"model\":model, \"fold\":fold, \"TP\":TP, \"TN\":TN, \"FP\":FP, \"FN\":FN}\n",
    "\n",
    "rslt_df = pd.Dataframe(rslt_list)\n",
    "rslt_df"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('my-rdkit-env': conda)"
  },
  "interpreter": {
   "hash": "aa570c18d4c79ce07a0608da35f26cf3e1b27f1e4f5f00a1b9401f1c1693be53"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}