{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Calling packages and assigning variables\n",
    "Here i call the necessary packages as well as assigning variables. Of note are the paths to collected data which will need to be changed for replication in another system"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# !pip install -e git+https://github.com/garydoranjr/misvm.git#egg=misvm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\n",
    "import misvm\n",
    "import rdkit\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tpot import TPOTClassifier\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MACCSkeys\n",
    "from mordred import Calculator, descriptors\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"/home/samuel/honours_redo/1-data/selected_molecules.csv\")\n",
    "metabolite_data = pd.read_csv(\"/home/samuel/honours_redo/publication_work/biotransformer_output_phaseII.csv\").append(pd.read_csv(\"/home/samuel/honours_redo/publication_work/biotransformer_output_cyp1.csv\"))\n",
    "\n",
    "# validation_data = pd.read_csv()\n",
    "# validation_metabolite_data = pd.read_csv()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cleaning Biotransformer data\n",
    "This section is to ensure that metabolites label the correct parent molecule as the root parent and not the direct one. Simply it replaces the \"parent molecule\" if that molecule is a metabolite and records the parent of said molecule."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def normalize_smiles(smi):\n",
    "    try:\n",
    "        smi_norm = Chem.MolToSmiles(Chem.MolFromSmiles(smi))\n",
    "        return smi_norm\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "metabolite_data['smiles'] = metabolite_data['SMILES'].apply(lambda x: normalize_smiles(x))\n",
    "metabolite_data = metabolite_data.dropna(axis=0,subset=['smiles'])\n",
    "\n",
    "def parent_finder(smi):\n",
    "    for parent in data['smiles']:\n",
    "        try:\n",
    "            if Chem.MolToSmiles(Chem.MolFromSmiles(smi)) == Chem.MolToSmiles(Chem.MolFromSmiles(parent)):\n",
    "                return parent\n",
    "        except:\n",
    "            continue\n",
    "    return \"No parent found\"\n",
    "\n",
    "metabolite_data['parent smiles'] = metabolite_data['Precursor SMILES'].apply(lambda x:parent_finder(x))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "RDKit ERROR: [03:52:42] Explicit valence for atom # 31 N, 4, is greater than permitted\n",
      "RDKit ERROR: [03:52:42] Explicit valence for atom # 8 N, 4, is greater than permitted\n",
      "RDKit ERROR: [03:52:42] Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "RDKit ERROR: [03:52:42] Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "RDKit ERROR: [03:52:42] Explicit valence for atom # 10 N, 4, is greater than permitted\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining functions\n",
    "These functions are used to execute the described actions on the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def build_mil_model(training_data,grouping_data,fingerprint,MIL,name):\n",
    "    i=0;print(i); i+=1\n",
    "    if not os.path.isfile(\"saved_models/\"+name):\n",
    "            [bags,labels] = generate_bags(training_data,grouping_data,fingerprint);print(i); i+=1\n",
    "            model = MIL;print(i); i+=1\n",
    "            model.fit(bags,labels);print(i); i+=1\n",
    "            print(i); i+=1\n",
    "            pickle.dump(model, open(\"saved_models/\"+name, 'wb'));print(i); i+=1\n",
    "    else:\n",
    "        print(name,\"is already built\")\n",
    "\n",
    "def build_ml_model(training_data,fingerprint,ML,name):\n",
    "    if not os.path.isfile(\"saved_models/\"+name):\n",
    "        [instances,labels] = get_ml_fingerprint(training_data, fingerprint)\n",
    "        tpot_optimisation = ML\n",
    "        tpot_optimisation.fit(instances,labels)\n",
    "        model = tpot_optimisation.fitted_pipeline_\n",
    "        pickle.dump(model, open(\"saved_models/\"+name, 'wb'))  \n",
    "    else:\n",
    "        print(name,\"is already built\")\n",
    "\n",
    "def test_mil_model(testing_data,grouping_data,fingerprint,name):\n",
    "    loaded_model = pickle.load(open(\"saved_models/\"+name, 'rb'))\n",
    "    [bags,true_labels] = generate_bags(testing_data,grouping_data,fingerprint)\n",
    "    predictions = loaded_model.predict(bags)\n",
    "    predicted_labels = list(map(pos_or_neg,predictions))\n",
    "    df = pd.DataFrame({\n",
    "        'predicted' : predictions,\n",
    "        'predicted labal' : predicted_labels,\n",
    "        'true label' : true_labels\n",
    "    })\n",
    "    df.to_csv(\"saved_tests/\"+name.split(\".\")[0]+\".csv\",index=False )\n",
    "\n",
    "def test_ml_model(testing_data,fingerprint,name):\n",
    "    loaded_model = pickle.load(open(\"saved_models/\"+name, 'rb'))\n",
    "    [instances,true_labels] = get_ml_fingerprint(testing_data, fingerprint)\n",
    "    predictions = loaded_model.predict(instances)\n",
    "    predicted_labels = list(map(pos_or_neg,predictions))\n",
    "    df = pd.DataFrame({\n",
    "        'predicted' : predictions,\n",
    "        'predicted labal' : predicted_labels,\n",
    "        'true label' : true_labels\n",
    "    })\n",
    "    df.to_csv(\"saved_tests/\"+name.split(\".\")[0]+\".csv\",index=False )\n",
    "\n",
    "def generate_bags(df,bt_df, funct):\n",
    "        bags = []; labels = []\n",
    "        for smile in df['smiles'].to_list():\n",
    "            wk_data =   bt_df[bt_df[\"parent smiles\"]==smile]\n",
    "            if not wk_data.empty:\n",
    "                wk_data = get_mil_fingerprint(wk_data, funct)\n",
    "                bags        += [np.array(wk_data['fp_list'].to_list())]\n",
    "                labels      += [df.loc[df['smiles'] == smile, 'Ames'].item()]\n",
    "        for i,x in enumerate(labels):\n",
    "            if x == 0:\n",
    "                labels[i] = -1\n",
    "        bags = np.array(bags)\n",
    "        labels = np.array(labels)\n",
    "        return [bags, labels]\n",
    "\n",
    "def get_mil_fingerprint(df, function):\n",
    "    fn = function\n",
    "    df1 = df.copy()\n",
    "    df1['fp_list'] = df1['smiles'].apply(lambda x: list(fn(Chem.MolFromSmiles(x))))\n",
    "    return df1\n",
    "\n",
    "def pos_or_neg(x):\n",
    "    if x>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def number_check(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return \"broken\"\n",
    "\n",
    "def get_ml_fingerprint(df, function):\n",
    "    if function != \"Morgan\":\n",
    "        fn = function\n",
    "        df1 = df.copy()\n",
    "        df1['fp_list'] = df1['smiles'].apply(lambda x: list(fn(Chem.MolFromSmiles(x))))\n",
    "        df1 = df1.dropna(axis = 1, how = 'any')\n",
    "\n",
    "        df2 = pd.DataFrame(df1['fp_list'].to_list())\n",
    "        df2 = df2.applymap(number_check).dropna(axis =1, how = \"any\")\n",
    "        df2 = df2.drop(columns=df2.columns[(df2 == 'broken').any()])\n",
    "        df1['fp_list'] = df2.values.tolist()\n",
    "        X = np.array(df1['fp_list'].to_list())\n",
    "        Y = np.array(df1['Ames'].to_list())\n",
    "        return [X,Y]\n",
    "    else:\n",
    "        radius = 3\n",
    "        df1 = df.copy()\n",
    "        df1['fp_list'] = df1['smiles'].apply(lambda x: list(AllChem.GetMorganFingerprint(Chem.MolFromSmiles(x),radius).ToBinary()))\n",
    "        df2 = pd.DataFrame(df1['fp_list'].to_list())\n",
    "        df2 = df2.applymap(number_check).dropna(axis =1, how = \"any\")\n",
    "        df2 = df2.drop(columns=df2.columns[(df2 == 'broken').any()])\n",
    "        df1['fp_list'] = df2.values.tolist()\n",
    "        X = np.array(df1['fp_list'].to_list())\n",
    "        Y = np.array(df1['Ames'].to_list())\n",
    "        return [X,Y]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building models\n",
    "Here the above functions are used to build models. This section can be altered to build additional models if desired"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def develop_models(training_data,training_data_metabolites,testing_data,testing_data_metabolites,suffix=\"\"):\n",
    "    tested_mils =  [[\"MICA\", misvm.MICA(max_iters=50,verbose=False)],     \n",
    "                [\"MISVM\", misvm.MISVM(kernel='linear', C=1.0, max_iters=50,verbose=False)],\n",
    "                ['SIL', misvm.SIL(verbose=False)],\n",
    "                ['NSK', misvm.NSK(verbose=False)],\n",
    "                ['sMIL', misvm.sMIL(verbose=False)]]\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    # Iterate over the two used fingerprints\n",
    "    for fp in [[\"MACCS\",MACCSkeys.GenMACCSKeys],[\"RDFP\",Chem.RDKFingerprint]]:\n",
    "        # Iterate over the used MILs\n",
    "        for mil in tested_mils:\n",
    "            # Build and test MIL model\n",
    "            build_mil_model(training_data=training_data,grouping_data=training_data_metabolites,name=fp[0]+\"_\"+mil[0]+suffix+\".sav\",MIL=mil[1],fingerprint=fp[1])\n",
    "            test_mil_model(testing_data=testing_data,grouping_data=testing_data_metabolites,name=fp[0]+\"_\"+mil[0]+suffix+\".sav\",fingerprint=fp[1])\n",
    "        # Build and test TPOT model\n",
    "        build_ml_model(training_data=training_data, ML = TPOTClassifier(generations=10, population_size=100, cv=5, random_state=42, verbosity=2),fingerprint=fp[1],name=fp[0]+suffix+'_tpot.sav')\n",
    "        test_ml_model(testing_data=testing_data,name=fp[0]+suffix+'_tpot.sav',fingerprint=fp[1])\n",
    "\n",
    "        # build_ml_model(training_data=training_data, ML = TPOTClassifier(generations=10, population_size=100, cv=5, random_state=42, verbosity=2),fingerprint=fp[1],name=fp[0]+\"_tpot\"+suffix+'.sav')\n",
    "        # test_ml_model(testing_data=testing_data,name=fp[0]+\"_tpot\"+suffix+'.sav',fingerprint=fp[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "## For precaution i save the indexes of the models into each crossvalidation. This is also usefull incase the code times out part way through, in addition to a set random state\n",
    "\n",
    "if os.path.isfile(\"train_test.csv\"):\n",
    "    print(\"Crossvalidation indices already determined in file: train_test.csv\")\n",
    "    print('To redo them please delete these files and for a different set please choose another random stat or \"none\" within the code block above')\n",
    "else:\n",
    "    skf = StratifiedKFold(n_splits=10, random_state=871923, shuffle=True)\n",
    "    First = True\n",
    "    for itr in range(1,11):\n",
    "        for fold, [train_index, test_index] in enumerate(skf.split(data,data['Ames'])):\n",
    "            fold +=1\n",
    "            if First:\n",
    "                test_df = pd.DataFrame({\"index\":test_index})\n",
    "                train_df = pd.DataFrame({\"index\":train_index})\n",
    "                test_df[\"group\"] = \"test\"; train_df[\"group\"] = \"train\"\n",
    "                test_df[\"fold\"] = fold; train_df[\"fold\"] = fold\n",
    "                test_df[\"iteration\"] = itr; train_df[\"iteration\"] = itr\n",
    "                train_test = test_df.append(train_df)\n",
    "                First = False\n",
    "            else:\n",
    "                test_df = pd.DataFrame({\"index\":test_index})\n",
    "                train_df = pd.DataFrame({\"index\":train_index})\n",
    "                test_df[\"group\"] = \"test\"; train_df[\"group\"] = \"train\"\n",
    "                test_df[\"fold\"] = fold; train_df[\"fold\"] = fold\n",
    "                test_df[\"iteration\"] = itr; train_df[\"iteration\"] = itr\n",
    "                train_test = train_test.append(test_df).append(train_df)\n",
    "    train_test.to_csv(\"train_test.csv\",index=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Crossvalidation indices already determined in file: train_test.csv\n",
      "To redo them please delete these files and for a different set please choose another random stat or \"none\" within the code block above\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "train_test = pd.read_csv(\"train_test.csv\")\n",
    "for iteration in train_test[\"iteration\"].unique():  \n",
    "    for fold in train_test[\"fold\"].unique():\n",
    "        train_index = train_test[(train_test[\"group\"] == \"train\") & (train_test[\"fold\"] == fold) & (train_test[\"iteration\"] == iteration)][\"index\"].to_list()\n",
    "        test_index = train_test[(train_test[\"group\"] == \"test\") & (train_test[\"fold\"] == fold) & (train_test[\"iteration\"] == iteration)][\"index\"].to_list()\n",
    "        train = data.iloc[train_index]\n",
    "        test =  data.iloc[test_index]\n",
    "        develop_models(train,metabolite_data,test,metabolite_data,suffix='_kfold'+str(fold))\n",
    "        print(\"Done Fold\", fold)\n",
    "    print(\"Done Iteration\", iteration)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "MACCS_MICA_kfold1.sav is already built\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-4-4ff6d5601f93>:57: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  bags = np.array(bags)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "MACCS_MISVM_kfold1.sav is already built\n",
      "0\n",
      "MACCS_SIL_kfold1.sav is already built\n",
      "0\n",
      "MACCS_NSK_kfold1.sav is already built\n",
      "0\n",
      "MACCS_sMIL_kfold1.sav is already built\n",
      "MACCS_kfold1_tpot.sav is already built\n",
      "0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-4-4ff6d5601f93>:57: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  bags = np.array(bags)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Validation\n",
    "Here the model results are assessed"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# develop_models(data,bt_data,validation_data,validation_metabolite_data,suffix=\"_validation\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Analysis\n",
    "Here the results of each fold are calculated as well as deviation within crossvalidation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def confusion_matrix(df):\n",
    "    TP = len(df[(df[\"predicted label\"] == 1) & (df[\"true label\"] == 1)])\n",
    "    TN = len(df[(df[\"predicted label\"] == 0) & (df[\"true label\"] == 0)])\n",
    "    FP = len(df[(df[\"predicted label\"] == 1) & (df[\"true label\"] == 0)])\n",
    "    FN = len(df[(df[\"predicted label\"] == 0) & (df[\"true label\"] == 1)])\n",
    "    return [TP,TN,FP,FN]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rslt_list = []\n",
    "\n",
    "for filename in os.listdir(\"/home/samuel/honours_redo/publication_work/saved_tests\"):\n",
    "    [TP,TN,FP,FN] = confusion_matrix(pd.read_csv(filename))\n",
    "    fingerprint = filename.split(\"_\")[0]\n",
    "    model = filename.split(\"_\")[0]\n",
    "    fold = filename.split(\"_\")[-1].split(\".\")[0]\n",
    "    rslt_list += {\"fingerprint\":fingerprint, \"model\":model, \"fold\":fold, \"TP\":TP, \"TN\":TN, \"FP\":FP, \"FN\":FN}\n",
    "\n",
    "rslt_df = pd.Dataframe(rslt_list)\n",
    "rslt_df"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('my-rdkit-env': conda)"
  },
  "interpreter": {
   "hash": "aa570c18d4c79ce07a0608da35f26cf3e1b27f1e4f5f00a1b9401f1c1693be53"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}